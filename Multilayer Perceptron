#!/usr/bin/env python3
"""
MLP Classifier Evaluation, ROC Analysis, and Confusion Matrix Visualisation

This integrated script performs the following tasks:
  - Loads and preprocesses a dataset from an Excel file.
  - Encodes categorical variables and splits the data into training and test sets.
  - Uses 10-fold cross-validation to evaluate an MLP classifier over a grid of hyperparameters.
  - Selects the best hyperparameters based on training accuracy and trains a final model.
  - Computes ROC curves (per-class, micro-, and macro-average) for the final model.
  - Plots the ROC curves.
  - Computes and visualises confusion matrices for both training and test sets,
    including additional performance metrics (recall, precision, and accuracy) as an extra row and column.

Author: Lianyan Li
Date: [22/02/2025]
"""

import pandas as pd
import time
import warnings
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, KFold, cross_val_predict
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                             confusion_matrix, roc_curve, auc)
from sklearn.preprocessing import LabelEncoder, label_binarize
from sklearn.exceptions import ConvergenceWarning

# -----------------------------------------------------------------------------
# Configure Plotting: Set Times New Roman as the font
# -----------------------------------------------------------------------------
plt.rcParams['font.family'] = 'Times New Roman'

# -----------------------------------------------------------------------------
# DATA LOADING AND PREPROCESSING
# -----------------------------------------------------------------------------
file_path = 'Data for 5th-component.xlsx'  # Replace with your actual file path
df = pd.read_excel(file_path)

# Encode categorical variables
label_encoders = {}
for column in df.columns:
    if df[column].dtype == 'object':
        le = LabelEncoder()
        df[column] = le.fit_transform(df[column])
        label_encoders[column] = le

# Separate features and target variable
X = df.iloc[:, :-1]  # All columns except the last one
y = df.iloc[:, -1]   # The last column

# Binarize the target variable for ROC curve generation
y_binarized = label_binarize(y, classes=range(len(y.unique())))

# Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# Binarize training and test targets for ROC analysis
y_train_binarized = label_binarize(y_train, classes=range(len(y.unique())))
y_test_binarized = label_binarize(y_test, classes=range(len(y.unique())))

# -----------------------------------------------------------------------------
# HYPERPARAMETER TUNING WITH CROSS-VALIDATION (MLP)
# -----------------------------------------------------------------------------
results = []
hidden_states = [100, 200, 300, 400, 500]
max_iterations = [100, 300, 500, 700]
learning_rates = [0.01, 0.001]

# Initialize 10-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# Evaluate different hyperparameter combinations
for hidden_layer in hidden_states:
    for max_iter in max_iterations:
        for lr in learning_rates:
            mlp_classifier = MLPClassifier(
                hidden_layer_sizes=(int(hidden_layer),),
                max_iter=int(max_iter),
                learning_rate_init=float(lr),
                random_state=42,
                early_stopping=True,
                n_iter_no_change=10
            )
            start_time = time.time()
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', ConvergenceWarning)
                y_train_pred = cross_val_predict(mlp_classifier, X_train, y_train, cv=kf)
            training_time = time.time() - start_time

            acc = accuracy_score(y_train, y_train_pred)
            prec = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)
            rec = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_train, y_train_pred, average='weighted')

            results.append({
                'hidden_layer': hidden_layer,
                'max_iter': max_iter,
                'learning_rate': lr,
                'accuracy': acc,
                'precision': prec,
                'recall': rec,
                'f1_score': f1,
                'training_time': training_time
            })

results_df = pd.DataFrame(results)
best_params = results_df.loc[results_df['accuracy'].idxmax()]
print("Best Hyperparameters:")
print(best_params)

# -----------------------------------------------------------------------------
# FINAL MODEL TRAINING & EVALUATION
# -----------------------------------------------------------------------------
final_mlp_classifier = MLPClassifier(
    hidden_layer_sizes=(int(best_params['hidden_layer']),),
    max_iter=int(best_params['max_iter']),
    learning_rate_init=float(best_params['learning_rate']),
    random_state=42,
    early_stopping=True,
    n_iter_no_change=10
)
final_mlp_classifier.fit(X_train, y_train)

# Predict on training and test sets
y_train_pred_final = final_mlp_classifier.predict(X_train)
y_test_pred_final = final_mlp_classifier.predict(X_test)

# Compute confusion matrices
train_conf_matrix = confusion_matrix(y_train, y_train_pred_final)
test_conf_matrix = confusion_matrix(y_test, y_test_pred_final)

# Predict probabilities on test set for ROC analysis
y_test_proba_final = final_mlp_classifier.predict_proba(X_test)

# -----------------------------------------------------------------------------
# ROC CURVE ANALYSIS
# -----------------------------------------------------------------------------
n_classes = y_binarized.shape[1]
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_test_proba_final[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Micro-average ROC
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_binarized.ravel(), y_test_proba_final.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Macro-average ROC
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Define colours and class labels for ROC plotting
colors = ['red', 'purple', 'orange', 'blue', 'green']
classes = ['F0', 'F1', 'F2', 'F3', 'F4']

plt.figure(figsize=(8, 6))
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {classes[i]} (area = {roc_auc[i]:0.2f})')
plt.plot(fpr["micro"], tpr["micro"], linestyle=':', linewidth=2,
         label=f'Micro-average ROC (area = {roc_auc["micro"]:0.2f})')
plt.plot(fpr["macro"], tpr["macro"], linestyle=':', linewidth=2,
         label=f'Macro-average ROC (area = {roc_auc["macro"]:0.2f})')
plt.plot([0, 1], [0, 1], '--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=18)
plt.ylabel('True Positive Rate', fontsize=18)
plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=20)
plt.legend(loc="lower right", fontsize=12)
plt.show()

# -----------------------------------------------------------------------------
# CONFUSION MATRIX VISUALISATION WITH EXTRA METRICS
# -----------------------------------------------------------------------------
def plot_conf_matrix(conf_matrix, title):
    """
    Plot a confusion matrix with an extra row and column that display per-class recall,
    per-class precision, and overall accuracy.
    """
    total = np.sum(conf_matrix)
    labels = [[f"{val:0.0f}\n{val / total:.2%}" for val in row] for row in conf_matrix]
    states = ['F0', 'F1', 'F2', 'F3', 'F4']

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=labels, cmap='Reds', fmt='',
                xticklabels=states, yticklabels=states, cbar=False, ax=ax,
                annot_kws={"size": 18})
    ax.set_title(title, fontweight='bold', fontsize=20)
    ax.tick_params(labeltop=True, labelbottom=False, length=0, labelsize=18)
    ax.set_ylabel('Actual Damage State', fontweight='bold', fontsize=19)
    ax.set_xlabel('Predicted Damage State', fontweight='bold', fontsize=19)

    # Create a matrix for extra metrics (recall, precision, and overall accuracy)
    f_mat = np.zeros((conf_matrix.shape[0] + 1, conf_matrix.shape[1] + 1))
    epsilon = 1e-10  # To avoid division by zero

    # Fill the extra column with per-class recall
    f_mat[:-1, -1] = np.diag(conf_matrix) / (np.sum(conf_matrix, axis=1) + epsilon)
    # Fill the extra row with per-class precision
    f_mat[-1, :-1] = np.diag(conf_matrix) / (np.sum(conf_matrix, axis=0) + epsilon)
    # Compute overall accuracy in the bottom-right cell
    f_mat[-1, -1] = np.trace(conf_matrix) / (total + epsilon)

    f_mask = np.ones_like(f_mat)
    f_mask[:, -1] = 0  # Unmask last column
    f_mask[-1, :] = 0  # Unmask last row

    f_color = np.ones_like(f_mat)
    f_color[-1, -1] = 0  # Different colour for overall accuracy

    f_annot = [[f"{val:0.2%}" for val in row] for row in f_mat]
    f_annot[-1][-1] = "Acc.:\n" + f_annot[-1][-1]

    sns.heatmap(f_color, mask=f_mask, annot=f_annot, fmt='',
                xticklabels=states + ["Recall"],
                yticklabels=states + ["Precision"],
                cmap=ListedColormap(['skyblue', 'lightgrey']),
                cbar=False, ax=ax,
                annot_kws={"size": 16})
    plt.show()

# Plot the training and testing confusion matrices with extra metrics
plot_conf_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_conf_matrix(test_conf_matrix, "Testing Confusion Matrix")

# -----------------------------------------------------------------------------
# CONSOLE OUTPUT
# -----------------------------------------------------------------------------
print("Performance Metrics (first few rows):")
print(results_df.head())
print("\nBest Parameters:")
print(best_params)
print("\nConfusion Matrix on Test Set:")
print(conf_matrix)
